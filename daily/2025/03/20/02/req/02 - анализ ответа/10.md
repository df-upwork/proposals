## 1.
Потенциальный клиент опубликовал на Upwork следующий проект:
### 1.1. Title
Azure Data Factory expert needed for immediate short task - must start today

### 1.2. Description
We have a set of seven short data pipelines taking data from one source and inserting it into another, using ADF. (Note classic ADF, not Fabric.)
Due to illness, our senior data engineer is not able to complete the work, and we need an urgent replacement. Documentation is thorough, and although the flows are a little complex, this should be a nice quick job for someone with ADF experience.
You must be highly proficient with ADF. Please provide examples of recent projects. You must be able to start work today.

### 1.3. Tags
azure data factory

### 1.5. Questions
#### 1.5.1.
Please confirm you have up-to-date, expert experience with classic ADF. Please do not apply if you cannot say yes to this requirement.
#### 1.5.2.
Describe your recent experience with similar projects
#### 1.5.3.
Please list any certifications related to this project

## 2. Информация о клиенте
### 2.1. Местоположение
United Kingdom
London

## 2.2. Характеристики компании
### 2.2.1. Сектор экономики
неизвестно
### 2.2.2. Количество сотрудников
Mid-sized company (10-99 people)

## 2.3. Характеристики учётной записи на Upwork
### 2.3.1. Member since
Mar 21, 2021
#### 2.3.2. Hire rate (%)
100
#### 2.3.3. Количество опубликованных проектов (jobs posted)
6
#### 2.3.4. Total spent
$83K
#### 2.3.5. Количество оплаченных часов в почасовых проектах
1990

## 3. Обозначения
### 3.1. `C`
Клиента буду называть `C`.

### 3.2. `P`
Проект `C` буду называть `P`.

### 3.3. `Ps`
Множество подобных `P` проектов буду обозначать `Ps`.

## 4. Ссылки на официальную документацию
### 4.1.
Я тебя спросил:
~~~
Дай 2 ссылки:
- на официальную документацию «classic ADF»
- на официальную документацию «Fabric ADF»
~~~

### 4.2.
Ты ответил:
~~~
**Ниже приведены 2 ссылки на официальную документацию от Microsoft по двум разным версиям ADF:**

1. **Classic ADF (Azure Data Factory)**  
   [Документация по Azure Data Factory](https://learn.microsoft.com/azure/data-factory/)  

2. **Fabric ADF (Data Factory в Microsoft Fabric)**  
   [Документация по Data Factory в Microsoft Fabric](https://learn.microsoft.com/fabric/data-factory/)
~~~

## 5. `ADF`
«Azure Data Factory» / «Classic ADF» (пункт 1 твоего ответа пункта 4.2) буду обозначать `ADF`.
https://learn.microsoft.com/azure/data-factory

## 6. `FDF`
«Data Factory в Microsoft Fabric» / «Fabric ADF»  (пункт 2 твоего ответа пункта 4.2) буду обозначать `FDF`.
https://learn.microsoft.com/fabric/data-factory

## 10. Мой потенциальный ответ клиенту (редакция 5)
### 10.1.
Я тебя спросил:
~~~~~~
## 10. Мой потенциальный ответ клиенту (редакция 5)
Я хочу ответить клиенту так:
~~~
Существуют 2 версии «classic ADF»: v1 и v2. В v2 ваша задача обычно решается так:
1) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»).
2) Указать метод аутентификации (например, Managed Identity, Service Principal, SQL Authentication и т. д.).
3) Если ваш источник или приёмник данных находится локально или за файерволом, необходимо:
3.1) Установить и настроить Self-hosted Integration Runtime (SHIR)
3.2) Сконфигурировать сетевые правила или дополнительные сертификаты для обеспечения защищённого соединения.
4) Создать «Datasets» для каждого источника и приёмника.
При необходимости, настроить их как параметризованные (dataset parameters), чтобы гибко задавать пути к файлам, имена таблиц и другие переменные.
5) Создать pipeline («Author» → «+» → «Pipeline»).
6) Добавить к pipeline «Activities» типа «Copy Data» или «Mapping Data Flow» (если требуется трансформация).
Для использования «Mapping Data Flow» необходимо настроить Integration Runtime так, чтобы в нём была включена поддержка Data Flow (в разделе «Manage» → «Integration runtimes»).
7) Настроить для pipeline входной и выходной «Dataset», указав:
7.1) Источник (например, «Azure SQL Database Dataset», REST API, Amazon S3 или локальную базу через Self-hosted Integration Runtime).
7.2) Приёмник (например, «Azure Blob Storage Dataset», Data Lake Storage или любую другую из более чем 90 поддерживаемых коннекций).
8) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
9) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook», «Web Activity», «Execute Pipeline», «ForEach» и так далее) с учётом потребностей проекта.
10) При работе с повторяющимися шаблонами копирования/трансформации или при необходимости гибко менять имена таблиц и путей к файлам, можно использовать pipeline parameters, dataset parameters и dynamic content expressions.
Например, в Dataset можно добавить параметры для путей к файлам или имён таблиц и затем подставлять их в зависимости от окружения или шага обработки.
11) Проверить и протестировать pipeline:
11.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
11.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
12) Добавить «Triggers» для автоматического запуска.
13) Подключить репозиторий Git (Azure DevOps или GitHub) для контроля версий и совместной разработки (если это ещё не сделано). 
Это упростит вашему разработчику (когда он поправится) отслеживание изменений в ADF, внесённых в период его болезни, а также позволит использовать CI/CD-процессы при переносе пайплайнов между средами.
14) Опубликовать все изменения.
Если планируется переносить решения между несколькими средами (Dev, Test, Prod) или массово параметризовать ADF-объекты, рекомендуется использовать подход автоматизированного развёртывания (ARM-шаблоны или Bicep) совместно с CI/CD (например, Azure DevOps или GitHub Actions). 
Это упростит перенос и обновление ваших «Pipeline», «Datasets» и «Linked Services» в разных окружениях без ручного дублирования.
15) Настроить мониторинг и оповещения. 
В разделе «Monitor» можно использовать встроенные Alert rules и Metrics (через Azure Monitor), чтобы автоматически отслеживать состояние пайплайнов и получать уведомления (по email, SMS и т. п.) при сбоях или отклонениях от нормальной работы.
~~~

## 11. Твоя задача
Есть ли в моём ответе фактические ошибки?
Есть ли в моём ответе логические ошибки?
Упустил ли я в ответе нечто важное?
~~~~~~

### 10.2.
Ты ответил:
#### 10.2.1. Ответ 1
~~~

~~~
#### 10.2.2. Ответ 2 (Grok)
~~~

~~~

## 11.
В пукте STUB своего ответа пукнкта 10.2.STUB ты писал:
```
STUB
```
Предложи конкретные правки к моей инструкции пункта 10.1 для устранения именно этого конкретного твоего замечания (не других).
Полностью инструкцию не переписывай: вместо этого предложи конкретные точечные правки.
Для подготовки правок используй свой ответ пункта 10.2.

1) I am currently developing an AI-based legal system where I have already tackled a similar challenge: a similar chatbot must, on the one hand, be based on a general-purpose LLM, and on the other, it must employ precise knowledge of a specific, narrow domain, which the underlying LLM does not possess sufficiently well, and sometimes does not know at all.
2) Today (end of December 2024), the best solution for your case (B2B customer support in eCommerce) is a combination of two technologies:
2.1) RAG (Retrieval Augmented Generation):
2.1.1) https://en.wikipedia.org/wiki/Retrieval-augmented_generation
2.1.2) https://help.openai.com/en/articles/8868588
2.1.3) https://www.anthropic.com/news/contextual-retrieval#a-primer-on-rag-scaling-to-larger-knowledge-bases
2.2) Function Calling (OpenAI) / Tools (Anthropic):
2.2.1) https://platform.openai.com/docs/guides/function-calling
2.2.2) https://docs.anthropic.com/en/docs/build-with-claude/tool-use
2.2.3) https://github.com/anthropics/courses/blob/master/tool_use/01_tool_use_overview.ipynb#repos-sticky-header
3) In your case, technologies 2.1 and 2.2 will work best when used together : RAG fetches the relevant data (e.g., B2B contractual terms, product specifications, inventory, etc.), while function calls enable the bot to operate «intelligently» within the scope of business logic (placing orders, calculating costs, checking discounts, etc.).
Combining Retrieval-Augmented Generation (RAG) with function calling indeed allows you, on the one hand, to provide the model with “knowledge” of specific documents (through RAG), and on the other, to make precise external API calls (through function calling) for dynamic operations such as checking warehouse stock or placing an order in Magento. 
This two-step approach (first retrieving the necessary information, then calling the relevant function) is currently the recommended method for building corporate chatbots that integrate with real-world services.
3.1) In B2B customer support in eCommerce, these technologies are usually combined as follows:
3.1.1) Step RAG: The LLM uses your «retriever» (or a built-in RAG approach) to obtain relevant documents (texts, product descriptions, legal regulations, etc.).
3.1.2) Step function calling: If you need to perform specific actions (for example, place an order or calculate shipping), the LLM «decides» that it is time to call a function like placeOrder(userId, itemId, quantity), and so on.
3.2) Example.
3.2.1) The user asks: «What is the price of product X in the B2B wholesale warehouse?»
3.2.2) AI model:
3.2.2.1) Through RAG, it finds the description of product items (price lists, technical documentation).
3.2.2.2) Then, using function calling, it invokes the Magento API to check inventory, the current price, discounts, and so on.
3.2.2.3) Finally, it replies, referencing both the retrieved data and the results of the function call.
4) How to implement RAG in your case?
4.1) Document your business knowledge in the formats commonly accepted today for LLM: Markdown, XML, PDF, images.
4.2) Convert the data from step 3.1 into embeddings:
4.2.1) https://en.wikipedia.org/wiki/Word_embedding
4.2.2) https://platform.openai.com/docs/guides/embeddings
4.2.3) https://docs.anthropic.com/en/docs/build-with-claude/embeddings
4.3) Save the embeddings in a vector database:
4.3.1) https://en.wikipedia.org/wiki/Vector_database
4.4) Retrieval: Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query
4.5) Augmentation: The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query
4.6) Generation: Finally, the LLM can generate output based on both the query and the retrieved documents
5) How to implement function calling in your case?
Below is an example of how we can address **point 5, “How to implement function calling in your case?”**, building on the points already mentioned (1–4). I’ve kept it detailed enough to show the depth of the solution, but it’s still explained in business-oriented terms:
5.1) **Identify the specific functions (API methods) the chatbot might call within your Magento environment.**  
   - Example functions could include:  
     - `getProductPrice(productId)` — returns the current price of a given product, taking wholesale discounts into account.  
     - `placeOrder(userId, productId, quantity)` — creates an order using Magento’s API.  
     - `getShippingCost(address, productId, quantity)` — calculates shipping costs.  
     - `updateCustomerInfo(userId, customerData)` — updates user information (phone, email, address).  
   - Essentially, each process you want to automate via chat gets its own defined function (an endpoint) with strict descriptions of input parameters and return data.

5.2) **Provide a formal description (e.g., JSON Schema) so the AI model “knows” which functions are available.**  
   - In OpenAI, this is done by passing an array of `functions` and `function_call` to the `chat/completions` endpoint.  
   - In Anthropic’s Claude, a similar mechanism (“Tools”) describes the structure of each tool and how to call it.  
   - For example, in OpenAI:
     ```json
     [
       {
         "name": "placeOrder",
         "description": "Create an order for the user",
         "parameters": {
           "type": "object",
           "properties": {
             "userId": { "type": "string" },
             "productId": { "type": "string" },
             "quantity": { "type": "number" }
           },
           "required": ["userId", "productId", "quantity"]
         }
       }
       ...
     ]
     ```

5.3) **Instruct the LLM (through the “prompt” or “context”) when to call each function.**  
   - For example: “If the user wants to place an order, call the `placeOrder` function with these parameters. Don’t generate a user-facing response at that step—only produce the JSON function call.”  
   - This way, whenever the model “understands” the user is requesting an order or another specific action, it creates the JSON describing the function call.

5.4) **Implement a “handler” in your server-side application to process these function calls.**  
   - When the LLM returns JSON, your system parses out the function name and parameters, then calls the corresponding Magento method.  
   - You then feed the result from Magento back to the LLM or directly to the user, depending on your workflow.

5.5) **Use Magento’s API (REST or GraphQL) to communicate with its backend.**  
   - For instance, you might call `placeOrder` (via REST) with the required parameters.  
   - After you receive Magento’s response (for instance, JSON with order status), you plug that data into the next LLM reply (or, if your logic requires, directly inform the user).

5.6) **Ensure error handling and authorization checks.**  
   - Even if the LLM “requests” a function call with invalid arguments, your code must validate them (permissions, inventory, etc.).  
   - If something fails—like lacking user rights or a discontinued product—return an error message to the LLM so it can respond appropriately.

5.7) **Example scenario**  
   - User says: “Please place an order for 100 units of SKU123 for my account user-567.”  
   - The chatbot (LLM) decides to call `placeOrder(userId, productId, quantity)` and generates JSON:
     ```json
     {
       "name": "placeOrder",
       "arguments": {
         "userId": "user-567",
         "productId": "SKU123",
         "quantity": 100
       }
     }
     ```
   - Your backend then calls the corresponding Magento REST endpoint, receives a response such as:
     ```json
     {
       "orderId": "ORD-9999",
       "orderStatus": "placed"
     }
     ```
   - This data is passed back to the LLM (through the function-calling response), and the chatbot can then confirm to the user: “Your order ORD-9999 has been placed successfully.”


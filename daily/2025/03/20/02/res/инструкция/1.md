1) Подготовить окружение в Azure
1.1) Войти в «Azure Portal».
1.2) Перейти в «Create a resource» → «Integration» → «Azure Data Factory».
1.3) Указать уникальное имя для ресурса ADF, выбрать нужный «Subscription», «Resource Group» и регион.
1.4) Нажать «Create» и дождаться окончания развертывания.

2) Настроить «Git integration» (при необходимости)
2.1) В ADF открыть раздел «Manage» (значок шестерёнки слева).
2.2) Перейти в «Source control» → «Git configuration».
2.3) Подключить репозиторий (Azure DevOps или GitHub), если требуется управление версиями и применение CI/CD.

3) Создать «Linked Services» для источника данных и приёмника
3.1) В разделе «Manage» → «Linked Services» нажать «+ New».
3.2) Выбрать нужный тип коннектора (например, «Azure SQL Database», «Azure Blob Storage», «Amazon S3» и т. д.).
3.3) Указать параметры подключения (строка соединения, ключи доступа, учётные данные). При необходимости использовать «Azure Key Vault», выбрав соответствующий параметр секрета.
3.4) Сохранить «Linked Service» и проверить «Connection» с помощью кнопки «Test connection».

4) Создать «Datasets» для каждого источника и приёмника
4.1) В левом меню перейти в «Author» (значок карандаша).
4.2) Нажать «+» → «Dataset».
4.3) Выбрать соответствующий «Linked Service» и настроить схему (таблица, путь к файлам, формат и т. д.).
4.4) Повторить для каждого набора данных (как входного, так и выходного).

5) Настроить «Integration Runtime», если требуется дополнительная производительность или подключение к локальным источникам
5.1) Перейти в «Manage» → «Integration Runtimes».
5.2) Выбрать «+ New» и указать тип:
    • «Azure» (автоматически управляемый)  
    • «Self-Hosted» (если необходимо подключаться к локальным ресурсам)  
5.3) При «Self-Hosted» варианте установить соответствующий агент на локальной машине и указать ключ.
5.4) Убедиться, что «Integration Runtime» доступен при выборе его в «Linked Service».

6) Создать «Pipeline»
6.1) В разделе «Author» нажать «+» → «Pipeline».
6.2) Добавить «Activities» типа «Copy Data» или «Data Flow» (если требуется трансформация).
6.3) Настроить входной и выходной «Dataset», указав:
    • Источник (например, «Azure SQL Database Dataset»).  
    • Приёмник (например, «Azure Blob Storage Dataset»).  
6.4) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
6.5) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook» и так далее) с учётом потребностей проекта P.
6.6) Повторить настройку в рамках одной «Pipeline» для нескольких цепочек «Activities» или создать отдельные «Pipelines», если проект P предусматривает 7 разных потоков.

7) Проверить и протестировать «Pipeline»
7.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
7.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
7.3) Убедиться в корректном переносе и/или трансформации данных: в источнике должна уменьшиться/измениться выборка (если операции чтения синхронные), в приёмнике должны появиться ожидаемые данные.

8) Добавить «Triggers» для автоматического запуска
8.1) Вернуться в «Author», открыть вкладку «Pipeline» и нажать «Add trigger».
8.2) Выбрать «New/Edit».
8.3) Указать тип: «Schedule», «Tumbling Window», «Event» (например, при появлении новых файлов), в зависимости от требований к проекту P.
8.4) Настроить расписание (время, интервал) или условия события.
8.5) Сохранить и опубликовать изменения.

9) Опубликовать все изменения
9.1) Нажать «Publish all» в верхнем меню.
9.2) Дождаться окончания процесса публикации. Изменения будут развернуты в рабочем окружении ADF.

10) Настроить мониторинг и оповещения
10.1) В разделе «Monitor» → «Alerts & metrics» при необходимости добавить правила оповещения (например, при неудачном выполнении «Pipeline»).
10.2) Проверять статус в «Pipeline runs» и «Activity runs». При необходимости использовать «Azure Monitor» для расширенного логирования.

11) Управление конфигурациями и DevOps (при большом объёме изменений)
11.1) При использовании «Git integration» выполнить commit и push изменений для командной работы и аудита версий.
11.2) Настроить «Release Pipeline» в Azure DevOps или GitHub Actions, если нужна автоматизация развертывания в разные среды (Dev, Test, Prod).
11.3) В случае необходимости экспортировать шаблоны (ARM Templates) для резервных копий или переноса.

12) Документация
12.1) Описать структуру «Pipeline» и «Datasets», включая настройку «Linked Services», расписание «Triggers» и основные параметры копирования.
12.2) Хранить файлы документации (например, `README.md`) в том же репозитории, что и проект ADF, или в любом другом средстве управления знаниями.

Данная инструкция охватывает базовую последовательность действий для проекта P в ADF (classic). При сложных сценариях (сложная трансформация, гибридные подключения, интеграция с другими сервисами Azure) возможно добавление дополнительных шагов в соответствии с официальной документацией ADF.
## 1.
Потенциальный клиент опубликовал на Upwork следующий проект:
### 1.1. Title
Azure Data Factory expert needed for immediate short task - must start today

### 1.2. Description
We have a set of seven short data pipelines taking data from one source and inserting it into another, using ADF. (Note classic ADF, not Fabric.)
Due to illness, our senior data engineer is not able to complete the work, and we need an urgent replacement. Documentation is thorough, and although the flows are a little complex, this should be a nice quick job for someone with ADF experience.
You must be highly proficient with ADF. Please provide examples of recent projects. You must be able to start work today.

### 1.3. Tags
azure data factory

### 1.5. Questions
#### 1.5.1.
Please confirm you have up-to-date, expert experience with classic ADF. Please do not apply if you cannot say yes to this requirement.
#### 1.5.2.
Describe your recent experience with similar projects
#### 1.5.3.
Please list any certifications related to this project

## 2. Информация о клиенте
### 2.1. Местоположение
United Kingdom
London

## 2.2. Характеристики компании
### 2.2.1. Сектор экономики
неизвестно
### 2.2.2. Количество сотрудников
Mid-sized company (10-99 people)

## 2.3. Характеристики учётной записи на Upwork
### 2.3.1. Member since
Mar 21, 2021
#### 2.3.2. Hire rate (%)
100
#### 2.3.3. Количество опубликованных проектов (jobs posted)
6
#### 2.3.4. Total spent
$83K
#### 2.3.5. Количество оплаченных часов в почасовых проектах
1990

## 3. Обозначения
### 3.1. `C`
Клиента буду называть `C`.

### 3.2. `P`
Проект `C` буду называть `P`.

### 3.3. `Ps`
Множество подобных `P` проектов буду обозначать `Ps`.

## 4. Ссылки на официальную документацию
### 4.1.
Я тебя спросил:
~~~
Дай 2 ссылки:
- на официальную документацию «classic ADF»
- на официальную документацию «Fabric ADF»
~~~

### 4.2.
Ты ответил:
~~~
**Ниже приведены 2 ссылки на официальную документацию от Microsoft по двум разным версиям ADF:**

1. **Classic ADF (Azure Data Factory)**  
   [Документация по Azure Data Factory](https://learn.microsoft.com/azure/data-factory/)  

2. **Fabric ADF (Data Factory в Microsoft Fabric)**  
   [Документация по Data Factory в Microsoft Fabric](https://learn.microsoft.com/fabric/data-factory/)
~~~

## 5. `ADF`
«Azure Data Factory» / «Classic ADF» (пункт 1 твоего ответа пункта 4.2) буду обозначать `ADF`.
https://learn.microsoft.com/azure/data-factory

## 6. `FDF`
«Data Factory в Microsoft Fabric» / «Fabric ADF»  (пункт 2 твоего ответа пункта 4.2) буду обозначать `FDF`.
https://learn.microsoft.com/fabric/data-factory

## 10. Мой потенциальный ответ клиенту (редакция 4)
### 10.1.
Я тебя спросил:
~~~~~~
## 10. Мой потенциальный ответ клиенту (редакция 4)
Я хочу ответить клиенту так:
~~~
I can do it today. Usually it is done as follows:
1) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»), указав метод аутентификации (например, Managed Identity, Service Principal, SQL Authentication и т. д.).
Если ваш источник или приёмник данных находится локально или за файерволом, необходимо установить и настроить Self-hosted Integration Runtime (SHIR).
2) Создать «Datasets» для каждого источника и приёмника.
При необходимости, настроить их как параметризованные (dataset parameters), чтобы гибко задавать пути к файлам, имена таблиц и другие переменные.
3) Создать «Pipeline»:
3.1) В разделе «Author» → «+» → «Pipeline».
3.2) Добавить «Activities» типа «Copy Data» или «Mapping Data Flow» (если требуется трансформация).
3.3) Настроить входной и выходной «Dataset», указав:
- Источник (например, «Azure SQL Database Dataset»).  
- Приёмник (например, «Azure Blob Storage Dataset»).  
3.4) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
3.5) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook» и так далее) с учётом потребностей проекта.
3.6) При работе с повторяющимися шаблонами копирования/трансформации или при необходимости гибко менять имена таблиц и путей к файлам, можно использовать pipeline parameters, dataset parameters и dynamic content expressions.
Например, в Dataset можно добавить параметры для путей к файлам или имён таблиц и затем подставлять их в зависимости от окружения или шага обработки.
4) Проверить и протестировать «Pipeline»:
4.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
4.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
5) Добавить «Triggers» для автоматического запуска.
6) Опубликовать все изменения.
7) Настроить мониторинг и оповещения.
~~~

## 11. Твоя задача
Есть ли в моём ответе фактические ошибки?
Есть ли в моём ответе логические ошибки?
Упустил ли я в ответе нечто важное?
~~~~~~

### 10.2.
Ты ответил:
#### 10.2.1. Ответ 1
~~~
1. **Упоминание «classic ADF» может ввести в заблуждение.** Формально есть ADF v1 и ADF v2, причём пошаговые действия, о которых вы говорите, относятся именно к v2. Если заказчик действительно работает не с Fabric, а с «классическим» ADF, обычно имеют в виду v2 (в отличие от ADF v1 без визуального интерфейса Mapping Data Flow).

2. **Mapping Data Flow требует настройки Data Flow Runtime в Integration Runtime.** В тексте упомянут «Mapping Data Flow» как вариант для трансформации, но не уточнено, что в самой «Integration Runtime» нужно разрешить (или создать отдельную) среду для Data Flows.

3. **Нет критических фактических или логических ошибок.** Общая последовательность (Linked Services → Datasets → Pipeline → Debug → Trigger → Publish) корректна для v2.

4. **Не упомянута важность Git-репозитория (Source Control) и CI/CD.** Обычно для проектов любого масштаба рекомендуется подключать ADF к Git (Azure DevOps или GitHub) для контроля версий и развёртывания.

5. **Можно уточнить про автоматизацию развёртывания.** Если проект будет переноситься между средами (Test/Prod) или нужно массово параметризовать, часто используют ARM-шаблоны, но это не обязательно, зависит от требований.
~~~
#### 10.2.2. Ответ 2 (Grok)
~~~
1. **Пункт 1: "Linked Services"**
   - Фактических ошибок нет, но упущена важная деталь: для некоторых источников данных (например, локальных или защищённых файерволом) может потребоваться дополнительная настройка, такая как установка сертификатов или настройка сетевых правил. Это особенно актуально для проекта клиента, если данные находятся за файерволом, как вы упомянули SHIR. Рекомендуется указать этот нюанс явно.

2. **Пункт 2: "Datasets"**
   - Описание корректно, ошибок нет. Упоминание параметризации — сильная сторона, так как это демонстрирует гибкость и ваш опыт.

3. **Пункт 3.3: Источники и приёмники**
   - Нет фактических ошибок, но примеры источников и приёмников ограничены («Azure SQL Database», «Azure Blob Storage»). ADF поддерживает широкий спектр источников и приёмников (базы данных, файлы, API и т. д.). Упоминание этой гибкости могло бы усилить ответ и показать клиенту возможности инструмента.

4. **Пункт 3.5: "Activities"**
   - Перечисленные активности («Lookup», «Stored Procedure», «Script», «Databricks Notebook») корректны, но список неполный. В ADF доступны и другие полезные активности, такие как «Web Activity», «Execute Pipeline», «ForEach», которые могут быть важны для сложной логики обработки данных, упомянутой клиентом. Это стоит добавить для полноты.

5. **Пункт 4.2: Проверка выполнения**
   - Описание точное, но упущена возможность настройки оповещений и мониторинга через Azure Monitor или другие инструменты. Это могло бы быть полезно клиенту для отслеживания состояния pipeline, особенно в срочном проекте.

6. **Пункт 6: Публикация изменений**
   - Фактически верно, но не упомянута возможность интеграции с Git или Azure DevOps для версионирования и управления изменениями. Это важно для контроля версий и совместной работы, что может быть актуально для компании среднего размера.

7. **Пункт 7: Мониторинг и оповещения**
   - Указано верно, но не уточнены встроенные возможности ADF, такие как «Alert rules» и «Metrics». Это могло бы показать клиенту, что вы знаете о встроенных инструментах автоматизации оповещений, что усилило бы ваш ответ.
~~~

## 11. «Не упомянута работа с параметрами и выражениями»
В пукте STUB своего ответа пукнкта 10.2.STUB ты писал:
```
STUB
```
Предложи конкретные правки к моей инструкции пункта 10.1 для устранения именно этого конкретного твоего замечания (не других).
Полностью инструкцию не переписывай: вместо этого предложи конкретные точечные правки.
Для подготовки правок используй свой ответ пункта 10.2.

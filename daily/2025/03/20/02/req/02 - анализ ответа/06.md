## 1.
Потенциальный клиент опубликовал на Upwork следующий проект:
### 1.1. Title
Azure Data Factory expert needed for immediate short task - must start today

### 1.2. Description
We have a set of seven short data pipelines taking data from one source and inserting it into another, using ADF. (Note classic ADF, not Fabric.)
Due to illness, our senior data engineer is not able to complete the work, and we need an urgent replacement. Documentation is thorough, and although the flows are a little complex, this should be a nice quick job for someone with ADF experience.
You must be highly proficient with ADF. Please provide examples of recent projects. You must be able to start work today.

### 1.3. Tags
azure data factory

### 1.5. Questions
#### 1.5.1.
Please confirm you have up-to-date, expert experience with classic ADF. Please do not apply if you cannot say yes to this requirement.
#### 1.5.2.
Describe your recent experience with similar projects
#### 1.5.3.
Please list any certifications related to this project

## 2. Информация о клиенте
### 2.1. Местоположение
United Kingdom
London

## 2.2. Характеристики компании
### 2.2.1. Сектор экономики
неизвестно
### 2.2.2. Количество сотрудников
Mid-sized company (10-99 people)

## 2.3. Характеристики учётной записи на Upwork
### 2.3.1. Member since
Mar 21, 2021
#### 2.3.2. Hire rate (%)
100
#### 2.3.3. Количество опубликованных проектов (jobs posted)
6
#### 2.3.4. Total spent
$83K
#### 2.3.5. Количество оплаченных часов в почасовых проектах
1990

## 3. Обозначения
### 3.1. `C`
Клиента буду называть `C`.

### 3.2. `P`
Проект `C` буду называть `P`.

### 3.3. `Ps`
Множество подобных `P` проектов буду обозначать `Ps`.

## 4. Ссылки на официальную документацию
### 4.1.
Я тебя спросил:
~~~
Дай 2 ссылки:
- на официальную документацию «classic ADF»
- на официальную документацию «Fabric ADF»
~~~

### 4.2.
Ты ответил:
~~~
**Ниже приведены 2 ссылки на официальную документацию от Microsoft по двум разным версиям ADF:**

1. **Classic ADF (Azure Data Factory)**  
   [Документация по Azure Data Factory](https://learn.microsoft.com/azure/data-factory/)  

2. **Fabric ADF (Data Factory в Microsoft Fabric)**  
   [Документация по Data Factory в Microsoft Fabric](https://learn.microsoft.com/fabric/data-factory/)
~~~

## 5. `ADF`
«Azure Data Factory» / «Classic ADF» (пункт 1 твоего ответа пункта 4.2) буду обозначать `ADF`.
https://learn.microsoft.com/azure/data-factory

## 6. `FDF`
«Data Factory в Microsoft Fabric» / «Fabric ADF»  (пункт 2 твоего ответа пункта 4.2) буду обозначать `FDF`.
https://learn.microsoft.com/fabric/data-factory

## 10. Мой потенциальный ответ клиенту (редакция 3)
### 10.1.
Я тебя спросил:
~~~~~~
## 10. Мой потенциальный ответ клиенту (редакция 3)
Я хочу ответить клиенту так:
~~~
I can do it today. Usually it is done as follows:
1) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»), указав метод аутентификации (например, Managed Identity, Service Principal, SQL Authentication и т. д.).
2) Создать «Datasets» для каждого источника и приёмника
3) Создать «Pipeline»:
3.1) В разделе «Author» → «+» → «Pipeline».
3.2) Добавить «Activities» типа «Copy Data» или «Mapping Data Flow» (если требуется трансформация).
3.3) Настроить входной и выходной «Dataset», указав:
- Источник (например, «Azure SQL Database Dataset»).  
- Приёмник (например, «Azure Blob Storage Dataset»).  
3.4) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
3.5) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook» и так далее) с учётом потребностей проекта.
3.6) При работе с повторяющимися шаблонами копирования/трансформации или при необходимости гибко менять имена таблиц и путей к файлам, можно использовать pipeline parameters, dataset parameters и dynamic content expressions.
4) Проверить и протестировать «Pipeline»:
4.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
4.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
5) Добавить «Triggers» для автоматического запуска.
6) Опубликовать все изменения.
7) Настроить мониторинг и оповещения.
~~~

## 11. Твоя задача
Есть ли в моём ответе фактические ошибки?
Есть ли в моём ответе логические ошибки?
Упустил ли я в ответе нечто важное?
~~~~~~

### 10.2.
Ты ответил:
~~~
1. **Linked Services и Self-hosted Integration Runtime (SHIR)**  
   Вы указали создание Linked Services с методами аутентификации, что правильно. Однако вы не упомянули, что для некоторых источников данных (например, локальных баз данных или ресурсов за файерволом) может потребоваться установка **Self-hosted Integration Runtime (SHIR)**. Это важный аспект, если проект предполагает работу с данными вне облака Azure.

2. **Datasets и параметризация**  
   Вы упомянули создание Datasets, но не указали, что они могут быть **параметризированы**. Параметризация Datasets позволяет гибко задавать пути к файлам или имена таблиц, что особенно полезно для повторяющихся шаблонов, о которых вы говорите позже в пункте 3.6.

3. **Mapping Data Flow как опция**  
   Вы упомянули "Copy Data" и "Mapping Data Flow" как активности в Pipeline. Это верно, но стоит отметить, что **Mapping Data Flow** используется для сложных трансформаций и требует дополнительных ресурсов (например, Spark-кластера). Для простого копирования данных, как указано в описании проекта (7 коротких конвейеров), чаще достаточно активности **Copy Data**, а Mapping Data Flow может быть избыточным.

4. **Triggers и их типы**  
   Вы упомянули добавление Triggers для автоматического запуска, но не уточнили, какие типы триггеров существуют в ADF (например, **Schedule Trigger**, **Tumbling Window Trigger**, **Event-based Trigger**). Это может быть важно для клиента, если у него есть конкретные требования к автоматизации.

5. **Публикация изменений и Git**  
   Вы указали необходимость публикации изменений, что верно для применения их в продакшене. Однако вы не упомянули возможность интеграции с **Git** для управления версиями, что является лучшей практикой в командной разработке и может быть полезно для клиента.
~~~

## 11. «Не упомянута работа с параметрами и выражениями»
В пункте 10.2 ты писал:
```
4. **Не упомянута работа с параметрами и выражениями**  
   - Многие проекты ADF используют параметризацию (pipeline parameters, dataset parameters) и выражения для динамической подстановки путей, имён таблиц, и т.д.  
   - Это не обязательно, но иногда клиенту важно знать, что вы умеете настраивать гибкую параметризацию.
```
Предложи конкретные правки к моей инструкции пункта 10.1 для устранения именно этого конкретного твоего замечания (не других).
Полностью инструкцию не переписывай: вместо этого предложи конкретные точечные правки.
Для подготовки правок используй свой ответ пункта 10.2.

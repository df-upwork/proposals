https://gemini.google.com/share/c5d510ff758a

## Cᛘ₁: Комбинаторный взрыв кэша (Cache Bloat)

### Суть
Фундаментальной причиной сбоев является отсутствие нормализации URL-адресов в конфигурации VCL, что приводит к созданию миллионов дубликатов объектов в памяти.
В условиях фасетной навигации (фильтры по размеру, цвету) и наличия рекламных меток (`gclid`, `utm`) каждый уникальный запрос сохраняется Varnish как отдельный объект.
Это вызывает экспоненциальный рост количества записей в кэше, вытесняя полезные данные и потребляя ресурсы на хранение ключей.

### Оценка
100

### Доводы за
В описании проекта прямо указано наличие множественных фильтров («users can filter by gender, size, color») и рекламного трафика.
Без явной сортировки параметров строки запроса Varnish по умолчанию считает ссылки `?a=1&b=2` и `?b=2&a=1` разными объектами.
Высокая посещаемость сайта (4 млн сессий) гарантирует постоянный приток уникальных комбинаций URL, которые невозможно эффективно кэшировать без очистки.
Жалоба клиента на деградацию производительности коррелирует с перегрузкой структур данных, отвечающих за поиск и вытеснение объектов (LRU).

### Доводы против
Отсутствуют, так как это стандартное поведение Varnish Cache, которое гарантированно приводит к исчерпанию ресурсов без специальной настройки VCL.

## Cᛘ₂: Неограниченное временное хранилище (Unbounded Transient Storage)

### Суть
Varnish использует специальное хранилище `Transient` для короткоживущих объектов и технических записей `hit-for-miss`, создаваемых при невозможности кэширования.
По умолчанию это хранилище использует системный аллокатор `malloc` и не имеет ограничения по максимальному объему потребляемой памяти.
При наплыве трафика с уникальными метками или Cookies, которые бэкенд помечает как `private`, Varnish бесконтрольно заполняет RAM этими временными записями до полного падения сервера (OOM).

### Оценка
98

### Доводы за
Официальная документация подтверждает, что если параметр `-s Transient` не задан явно, используется «unbound malloc storage».
Симптомы, описанные клиентом («changes haven't helped»), указывают на то, что лимитирование основного хранилища не влияет на источник утечки.
Рекламный трафик генерирует массовое создание объектов `hit-for-miss` для предотвращения блокировок очередей запросов.
Этот механизм является «тихим убийцей» памяти, так как он работает параллельно с основным кэшем и часто игнорируется при настройке.

### Доводы против
Существует незначительная вероятность, что в используемом клиентом дистрибутиве (пакете) настройки по умолчанию изменены на безопасные.

## Cᛘ₃: Скрытые накладные расходы на метаданные (Metadata Overhead)

### Суть
Параметр конфигурации `-s malloc,SIZE` ограничивает только объем памяти для тела объектов, но игнорирует затраты на их метаданные.
Каждый объект в кэше требует около 1 КБ оперативной памяти для служебных структур (`struct obj`, `objcore`), хранящихся вне лимитируемой области.
Из-за проблемы `Cᛘ₁` количество объектов может достигать десятков миллионов, что приводит к потреблению десятков гигабайт памяти исключительно на метаданные.

### Оценка
95

### Доводы за
Математическая модель показывает, что 15 миллионов мелких объектов займут около 15 ГБ RAM сверх выделенного лимита хранилища.
Для сервера с 32 ГБ RAM такой неучтенный расход является критическим и неизбежно ведет к исчерпанию физической памяти.
Клиент демонстрирует непонимание этого механизма, полагая, что Varnish должен укладываться в заданные конфигом рамки.
Это объясняет «постоянный рост памяти» по мере наполнения кэша новыми уникальными ссылками.

### Доводы против
Данная проблема является вторичной и проявляется только при аномально большом количестве объектов в кэше.

## Cᛘ₄: Фрагментация памяти (Heap Fragmentation)

### Суть
Стандартный системный аллокатор `glibc`, используемый в Linux по умолчанию, неэффективен для многопоточной работы Varnish с частым выделением памяти.
Это приводит к внешней фрагментации кучи, когда операционная система считает память занятой процессом, хотя внутри она свободна, но разбита на мелкие участки.
На архитектуре AWS Graviton (ARM64) проблема фрагментации `glibc` проявляется особенно остро, вызывая рост RSS процесса.

### Оценка
90

### Доводы за
Инфраструктура клиента работает на процессорах Graviton, где выбор аллокатора критичен для производительности.
Многие стандартные образы OS не включают оптимизированный аллокатор `jemalloc` по умолчанию.
Сообщество пользователей Varnish подтверждает, что переход на `jemalloc` снижает потребление памяти на 30-50% за счет дефрагментации.
Симптомы «утечки», которую невозможно устранить настройками VCL, полностью соответствуют картине фрагментации памяти.

### Доводы против
Некоторые современные облачные образы (AMI) могут иметь предустановленный и настроенный `jemalloc`.

## Cᛘ₅: Неоптимальный размер страниц памяти (Page Size Mismatch)

### Суть
Процессоры AWS Graviton показывают значительно лучшую производительность при использовании страниц памяти размером 64 КБ (64k pages).
Стандартные ядра Linux используют страницы 4 КБ, что при больших объемах RAM увеличивает нагрузку на буфер трансляции адресов (TLB).
Это приводит к дополнительным накладным расходам и деградации производительности при произвольном доступе к памяти.

### Оценка
75

### Доводы за
Официальные рекомендации AWS для Graviton настоятельно советуют использовать 64k pages для нагрузок типа кэширования.
Использование 4k pages увеличивает размер таблиц страниц в памяти и количество промахов TLB.
Это объясняет часть проблем с производительностью, на которые жалуется клиент.

### Доводы против
Данный фактор влияет на производительность и латентность, но редко является прямой причиной аварийного завершения процесса из-за нехватки памяти.

## Вердикт

Проблема `P†` вызвана системным сочетанием критических архитектурных упущений.
Первопричиной является **Cache Bloat (`Cᛘ₁`)**, генерирующий миллионы паразитных объектов из-за отсутствия нормализации URL.
Это запускает механизмы **Metadata Overhead (`Cᛘ₃`)** и **Unbounded Transient Storage (`Cᛘ₂`)**, которые бесконтрольно потребляют оперативную память сверх установленных лимитов.
Ситуация усугубляется использованием неэффективного аллокатора **glibc (`Cᛘ₄`)**, который фрагментирует оставшиеся ресурсы.
Для решения задачи необходимо внедрить нормализацию запросов, жестко ограничить временное хранилище и переключиться на аллокатор `jemalloc`.
1) I am currently developing an AI-based legal system where I have already tackled a similar challenge: подобный чатбот должен, с одной стороны, базироваться на LLM общего назначения, а с другой — использовать точные знания конкретной узкой предметной области, которыми лежащая в основе LLM владеет недостаточно хорошо, а порой даже вообще о них ничего не знает.
2) На сегодня (конец декабря 2024 года) лучшим общепринятым решением этого challenge является технология RAG (Retrieval Augmented Generation): 
https://en.wikipedia.org/wiki/Retrieval-augmented_generation
https://help.openai.com/en/articles/8868588
https://www.anthropic.com/news/contextual-retrieval#a-primer-on-rag-scaling-to-larger-knowledge-bases
3) Таким образом, лучший способ вашей задачи таков:
3.1) Задокументировать знания вашего бизнеса в общепринятых на сегодня для LLM форматах: Markdown, XML, PDF, картинки.
3.2) Сконвертировать данные пункта 3.1 в embeddings: 
https://en.wikipedia.org/wiki/Word_embedding
https://platform.openai.com/docs/guides/embeddings
https://docs.anthropic.com/en/docs/build-with-claude/embeddings
3.3) Сохранить embeddings в vector database:
https://en.wikipedia.org/wiki/Vector_database 
3.4) Retrieval: Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query
3.5) Augmentation: The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query
3.6) Generation: Finally, the LLM can generate output based on both the query and the retrieved documents
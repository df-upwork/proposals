1) I am currently developing an AI-based legal system where I have already tackled a similar challenge: a similar chatbot must, on the one hand, be based on a general-purpose LLM, and on the other, it must employ precise knowledge of a specific, narrow domain, which the underlying LLM does not possess sufficiently well, and sometimes does not know at all.
2) As of today (the end of December 2024), the best commonly accepted solution to this challenge is the RAG (Retrieval Augmented Generation) technology:
2.1) https://en.wikipedia.org/wiki/Retrieval-augmented_generation
2.2) https://help.openai.com/en/articles/8868588
2.3) https://www.anthropic.com/news/contextual-retrieval#a-primer-on-rag-scaling-to-larger-knowledge-bases
3) Therefore, the best approach for your task is as follows:
3.1) Document your business knowledge in the formats commonly accepted today for LLM: Markdown, XML, PDF, images.
3.2) Convert the data from step 3.1 into embeddings:
3.2.1) https://en.wikipedia.org/wiki/Word_embedding
3.2.2) https://platform.openai.com/docs/guides/embeddings
3.2.3) https://docs.anthropic.com/en/docs/build-with-claude/embeddings
3.3) Save the embeddings in a vector database:
3.3.1) https://en.wikipedia.org/wiki/Vector_database
3.4) Retrieval: Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query
3.5) Augmentation: The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query
3.6) Generation: Finally, the LLM can generate output based on both the query and the retrieved documents
## 1.
Потенциальный клиент опубликовал на Upwork следующий проект:
### 1.1. Title
Azure Data Factory expert needed for immediate short task - must start today

### 1.2. Description
We have a set of seven short data pipelines taking data from one source and inserting it into another, using ADF. (Note classic ADF, not Fabric.)
Due to illness, our senior data engineer is not able to complete the work, and we need an urgent replacement. Documentation is thorough, and although the flows are a little complex, this should be a nice quick job for someone with ADF experience.
You must be highly proficient with ADF. Please provide examples of recent projects. You must be able to start work today.

### 1.3. Tags
azure data factory

### 1.5. Questions
#### 1.5.1.
Please confirm you have up-to-date, expert experience with classic ADF. Please do not apply if you cannot say yes to this requirement.
#### 1.5.2.
Describe your recent experience with similar projects
#### 1.5.3.
Please list any certifications related to this project

## 2. Информация о клиенте
### 2.1. Местоположение
United Kingdom
London

## 2.2. Характеристики компании
### 2.2.1. Сектор экономики
неизвестно
### 2.2.2. Количество сотрудников
Mid-sized company (10-99 people)

## 2.3. Характеристики учётной записи на Upwork
### 2.3.1. Member since
Mar 21, 2021
#### 2.3.2. Hire rate (%)
100
#### 2.3.3. Количество опубликованных проектов (jobs posted)
6
#### 2.3.4. Total spent
$83K
#### 2.3.5. Количество оплаченных часов в почасовых проектах
1990

## 3. Обозначения
### 3.1. `C`
Клиента буду называть `C`.

### 3.2. `P`
Проект `C` буду называть `P`.

### 3.3. `Ps`
Множество подобных `P` проектов буду обозначать `Ps`.

## 4. Ссылки на официальную документацию
### 4.1.
Я тебя спросил:
~~~
Дай 2 ссылки:
- на официальную документацию «classic ADF»
- на официальную документацию «Fabric ADF»
~~~

### 4.2.
Ты ответил:
~~~
**Ниже приведены 2 ссылки на официальную документацию от Microsoft по двум разным версиям ADF:**

1. **Classic ADF (Azure Data Factory)**  
   [Документация по Azure Data Factory](https://learn.microsoft.com/azure/data-factory/)  

2. **Fabric ADF (Data Factory в Microsoft Fabric)**  
   [Документация по Data Factory в Microsoft Fabric](https://learn.microsoft.com/fabric/data-factory/)
~~~

## 5. `ADF`
«Azure Data Factory» / «Classic ADF» (пункт 1 твоего ответа пункта 4.2) буду обозначать `ADF`.
https://learn.microsoft.com/azure/data-factory

## 6. `FDF`
«Data Factory в Microsoft Fabric» / «Fabric ADF»  (пункт 2 твоего ответа пункта 4.2) буду обозначать `FDF`.
https://learn.microsoft.com/fabric/data-factory

## 10. Мой ответ клиенту
Я хочу ответить клиенту так:
~~~
Существуют 2 версии «classic ADF»: v1 и v2. В v2 ваша задача обычно решается так:
1) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»).
2) Указать метод аутентификации (например, Managed Identity, Service Principal, SQL Authentication и т. д.).
3) Если ваш источник или приёмник данных находится локально или за файерволом, необходимо:
3.1) Установить и настроить Self-hosted Integration Runtime (SHIR)
3.2) Сконфигурировать сетевые правила или дополнительные сертификаты для обеспечения защищённого соединения.
4) Создать «Datasets» для каждого источника и приёмника.
При необходимости, настроить их как параметризованные (dataset parameters), чтобы гибко задавать пути к файлам, имена таблиц и другие переменные.
5) Создать pipeline («Author» → «+» → «Pipeline»).
6) Добавить к pipeline «Activities» типа «Copy Data» или «Mapping Data Flow» (если требуется трансформация).
Для использования «Mapping Data Flow» необходимо настроить Integration Runtime так, чтобы в нём была включена поддержка Data Flow (в разделе «Manage» → «Integration runtimes»).
7) Настроить для pipeline входной и выходной «Dataset», указав:
7.1) Источник (например, «Azure SQL Database Dataset», REST API, Amazon S3 или локальную базу через Self-hosted Integration Runtime).
7.2) Приёмник (например, «Azure Blob Storage Dataset», Data Lake Storage или любую другую из более чем 90 поддерживаемых коннекций).
8) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
9) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook», «Web Activity», «Execute Pipeline», «ForEach» и так далее) с учётом потребностей проекта.
10) При работе с повторяющимися шаблонами копирования/трансформации или при необходимости гибко менять имена таблиц и путей к файлам, можно использовать pipeline parameters, dataset parameters и dynamic content expressions.
Например, в Dataset можно добавить параметры для путей к файлам или имён таблиц и затем подставлять их в зависимости от окружения или шага обработки.
11) Проверить и протестировать pipeline:
11.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
11.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
12) Добавить «Triggers» для автоматического запуска.
13) Подключить репозиторий Git (Azure DevOps или GitHub) для контроля версий и совместной разработки (если это ещё не сделано). 
Это упростит вашему разработчику (когда он поправится) отслеживание изменений в ADF, внесённых в период его болезни, а также позволит использовать CI/CD-процессы при переносе пайплайнов между средами.
14) Опубликовать все изменения.
Если планируется переносить решения между несколькими средами (Dev, Test, Prod) или массово параметризовать ADF-объекты, рекомендуется использовать подход автоматизированного развёртывания (ARM-шаблоны или Bicep) совместно с CI/CD (например, Azure DevOps или GitHub Actions). 
Это упростит перенос и обновление ваших «Pipeline», «Datasets» и «Linked Services» в разных окружениях без ручного дублирования.
15) Настроить мониторинг и оповещения. 
В разделе «Monitor» можно использовать встроенные Alert rules и Metrics (через Azure Monitor), чтобы автоматически отслеживать состояние пайплайнов и получать уведомления (по email, SMS и т. п.) при сбоях или отклонениях от нормальной работы.
~~~

## 11. Твоя задача
Переведи ответ на английский.

## 12. Правила перевода
### 12.1.
Переводи именно в той стилистике, как написано на русском языке.
Не делай перевод более вежливым, чем оригинал.

### 12.2.
Те предложения, которые сейчас полностью на английском — оставь без изменения.

### 12.3.
Не используй Markdown: только plain text.
При этом можно и нужно использовать то форматирование, которое уже есть в оригинале: его не убирай.

### 12.4.
Форматируй перевод в точности как оригинал. 
В частности:
*) каждый абзац должен содержать ровно одно предложение
*) между абзацами не должно оставаться пустых строк.
*) кавычки используй те же, что и в оригинале: «» и ``.

### 12.5.
Не используй сокращения типа «don't». Все подобные фразы пиши полностью: «do not».

### 12.6.
Не используй жаргон.
Вместо этого используй официальные термины.
#### 12.6.1.
В частности, фразы в кавычках используй только в том случае, когда они являются точными цитатами.
Не используй фразы в кавычках для применения жаргонных фраз.
Например, следующий фрагмент текста недопустим, потому что там используется жаргонная фраза «пролетел»: 
```
Например, код, который пушит данные о покупке, подключён асинхронно и загружается с небольшой задержкой, а триггер уже «пролетел».
```

### 12.7.
При обсуждении программного обеспечения используй точные официальные термины на английском языке: именно в том виде, как они указаны в официальной англоязычной документации к этому программному обеспечению.

### 12.8.
Не используй «you need» и другие подобные обращённые к клиенту фразы, перекладывающие действия на него.
Помни: я пишу клиенту или потенциальному клиенту.
Делать в любом случае буду я, а не клиент.
Вместо «you need» используй 2 альтернативы:
#### 12.8.1.
Нейтральные фразы типа «it is necessary».
#### 12.8.2.
Глаголы в неопределённой форме.
Например, во фрагменте ниже использованы подобные глаголы «set up», «create»:
```
1.2) Set up the transfer of login events from WordPress to Power BI using Fabric / OneLake.
1.2.1) Set up a «Data Pipeline» from the WordPress database table that stores login events (see point 1.1) to Fabric / OneLake.
1.2.2) Set up a connection from Power BI to Fabric / OneLake to pass login events.
1.3) Create the data model in Power BI.
```
Обрати внимание, в этом фрагменте не говорится, кто именно будет выполнять описанные действия: ответственность не перекладывается на клиента, в отличие от «you need».

### 12.9.
Никогда не переводи понятие «сайт» / «веб-сайт» как «site». 
Вместо этого используй форму «website»: это является более профессиональным.

### 12.10.
Никогда не переводи понятие «пункт нумерованного списка» как «item».
Всегда переводи это как «point».
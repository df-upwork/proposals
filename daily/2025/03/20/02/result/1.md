Существуют 2 версии «classic ADF»: v1 и v2. В v2 ваша задача обычно решается так:
1) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»).
2) Указать метод аутентификации (например, Managed Identity, Service Principal, SQL Authentication и т. д.).
3) Если ваш источник или приёмник данных находится локально или за файерволом, необходимо:
3.1) Установить и настроить Self-hosted Integration Runtime (SHIR)
3.2) Сконфигурировать сетевые правила или дополнительные сертификаты для обеспечения защищённого соединения.
4) Создать «Datasets» для каждого источника и приёмника.
При необходимости, настроить их как параметризованные (dataset parameters), чтобы гибко задавать пути к файлам, имена таблиц и другие переменные.
5) Создать pipeline («Author» → «+» → «Pipeline»).
6) Добавить к pipeline «Activities» типа «Copy Data» или «Mapping Data Flow» (если требуется трансформация).
Для использования «Mapping Data Flow» необходимо настроить Integration Runtime так, чтобы в нём была включена поддержка Data Flow (в разделе «Manage» → «Integration runtimes»).
7) Настроить для pipeline входной и выходной «Dataset», указав:
7.1) Источник (например, «Azure SQL Database Dataset», REST API, Amazon S3 или локальную базу через Self-hosted Integration Runtime).
7.2) Приёмник (например, «Azure Blob Storage Dataset», Data Lake Storage или любую другую из более чем 90 поддерживаемых коннекций).
8) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
9) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook», «Web Activity», «Execute Pipeline», «ForEach» и так далее) с учётом потребностей проекта.
10) При работе с повторяющимися шаблонами копирования/трансформации или при необходимости гибко менять имена таблиц и путей к файлам, можно использовать pipeline parameters, dataset parameters и dynamic content expressions.
Например, в Dataset можно добавить параметры для путей к файлам или имён таблиц и затем подставлять их в зависимости от окружения или шага обработки.
11) Проверить и протестировать pipeline:
11.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
11.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
12) Добавить «Triggers» для автоматического запуска.
13) Подключить репозиторий Git (Azure DevOps или GitHub) для контроля версий и совместной разработки (если это ещё не сделано). 
Это упростит вашему разработчику (когда он поправится) отслеживание изменений в ADF, внесённых в период его болезни, а также позволит использовать CI/CD-процессы при переносе пайплайнов между средами.
14) Опубликовать все изменения.
Если планируется переносить решения между несколькими средами (Dev, Test, Prod) или массово параметризовать ADF-объекты, рекомендуется использовать подход автоматизированного развёртывания (ARM-шаблоны или Bicep) совместно с CI/CD (например, Azure DevOps или GitHub Actions). 
Это упростит перенос и обновление ваших «Pipeline», «Datasets» и «Linked Services» в разных окружениях без ручного дублирования.
15) Настроить мониторинг и оповещения. 
В разделе «Monitor» можно использовать встроенные Alert rules и Metrics (через Azure Monitor), чтобы автоматически отслеживать состояние пайплайнов и получать уведомления (по email, SMS и т. п.) при сбоях или отклонениях от нормальной работы.
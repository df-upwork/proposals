Если кратко: нужно **разбить большую задачу** на отдельные небольшие порции (batch processing), использовать **OpenAI API** (вместо диалогового интерфейса ChatGPT), и в каждом запросе к API **передавать чёткую инструкцию** вместе с небольшой частью списка, а результат складывать назад в общий пул. 

Ниже пошаговый **алгоритм**:

---

## 1. Сбор и первичная подготовка данных

1. **Экспорт поиска**  
   - Выгружаем все поисковые запросы из Google Ads (Search Terms Report) в CSV/Excel или другую удобную форму.  
   - Убеждаемся, что каждая строка (поисковый запрос) — отдельная запись.

2. **Очистка и фильтрация**  
   - Удаляем точные дубликаты.  
   - (Опционально) приводим к нижнему регистру, вычищаем артефакты (лишние пробелы, неинформативные спецсимволы и т. п.).

3. **Оценка объёма**  
   - Считаем общее число строк (N).  
   - Готовим структуру данных, из которой удобно брать эти строки порциями (batchами).

---

## 2. Разбиение на батчи (batch processing)

1. **Выбор размера батча**  
   - Определяем, сколько строк помещаем в один запрос к OpenAI. Обычно это 50–200 запросов, но зависит от средней длины каждого запроса.  
   - Не забываем, что у API есть лимит по токенам (нужно учитывать и текст нашей инструкции, и сами поисковые фразы).

2. **Создание очереди батчей**  
   - Последовательно нарезаем весь список на кусочки (batched_data[1], batched_data[2] и т. д.).  
   - Если есть необходимость ускорить, часть батчей можно обрабатывать параллельно — с учётом rate limit OpenAI.

---

## 3. Формирование промпта (prompt) для OpenAI API

1. **Инструкция**  
   - При вызове API передаём инструкции, например (упрощённый пример структуры):
     ```text
     System message: "Ты — помощник по анализу поисковых запросов."
     User message: 
       "Ниже приведён список поисковых запросов (до 200 шт).
        Твоя задача — определить, какие из них нерелевантны для локальных и 
        междугородних переездов (США, из/в Шарлотт, NC) и основаны ли 
        на конкурентах, DIY, скидках, малых работах и т.д.
        В итоге сформируй список минус-слов (однословных или фраз) в формате:
        - одно понятие на строку
        - без запятых
        - учти опечатки/синонимы
        Не пропускай ни один запрос."
     ```
   - Далее в том же сообщении прикрепляем блоком ~50–200 поисковых запросов.

2. **Важно**  
   - Чётко прописать **критерии**, когда запрос считается нерелевантным (гео-вне США, DIY, «cheap», упоминание конкурентов, про ремонт авто вместо переезда и т. п.).  
   - Сказать, чтобы GPT **не обрезал** список и не давал общих советов, а **обработал каждый запрос**.  

3. **Подстановка данных**  
   - Для каждого батча в `user message` дописываем соответствующий список (одним блоком текста).

---

## 4. Получение ответа и парсинг

1. **Ответ GPT**  
   - В ответе будет перечень минус-слов. Возможен формат наподобие:  
     ```
     - pet mover
     - railcar
     - honda crv
     - etc.
     ```
2. **Парсинг**  
   - Программно извлекаем каждый пункт из ответа:  
     - Отрезаем любые вступительные/заключительные слова GPT.  
     - Разбиваем по переводу строки (`\n`) или по дефису.  
   - Получаем «сырая_фраза_1», «сырая_фраза_2» и т. д.

3. **Сбор всех батчей**  
   - Все итоги (минус-слова/фразы) из каждого батча сводим в один большой список.

4. **Устранение дубликатов**  
   - Удаляем точные дубли.  
   - (Опционально) приводим к одному формату — например, в нижнем регистре.

---

## 5. Расширение (опционально)

1. **Синонимы и опечатки**  
   - Чтобы охватить близкие варианты написания, можно дополнительно попросить GPT сгенерировать синонимы/опечатки для каждого выявленного слова/фразы.  
   - Или использовать Embeddings (через OpenAI API) для поиска похожих терминов, проверяя их на нерелевантность.

2. **Фильтр ложных срабатываний**  
   - Ручная проверка части предложенных минус-слов, чтобы случайно не отрезать нужные поиски.

---

## 6. Итоговая выгрузка в Google Ads

1. **Формат списка**  
   - В итоге получаем единый текстовый файл или CSV со строками вида:
     ```
     cheap
     diy
     competitor brand
     ...
     ```
2. **Добавление в Ads**  
   - Через Google Ads interface (manual upload) или автоматически (Google Ads API/скрипты) загружаем минус-слова в нужные кампании/группы.

3. **Периодичность**  
   - Каждый месяц/неделю обновляем список, подгружаем новые поисковые термы и прогоняем через ту же процедуру.

---

## 7. Контроль качества

1. **Тестовые проверки**  
   - Перед массовым использованием можно отследить:  
     - Не упал ли объём кликов  
     - Не теряем ли нужных клиентов  
     - Идут ли желаемые конверсии и т. д.  

2. **Логирование**  
   - Сохраняем копии запросов-ответов GPT для анализа в случае спорных ситуаций.

3. **Дополнительная ML-модель** (при очень больших масштабах)  
   - Можно обучить локальную модель на датасете (с пометками «нерелевантно/релевантно»), чтобы 80–90% очевидных нерелевантных запросов отбрасывались без обращения к GPT. GPT используется как «тюнинг» для сложных случаев и генерации человекопонятных минус-слов.

---

### Итог

Таким образом, **ключ** к решению — это батч-обработка данных через OpenAI API (с учётом лимитов по токенам), пошаговый анализ поисковых запросов с понятным промптом, сбор и очистка полученных минус-слов и регулярное добавление их в Google Ads. Такой пайплайн **масштабируем**, **гибок** и позволяет автоматически поддерживать актуальный список нерелевантных запросов.
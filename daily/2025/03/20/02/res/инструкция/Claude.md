# Пошаговая инструкция по реализации проекта в Azure Data Factory

## Подготовка к работе

1. Войти в Azure Portal (https://portal.azure.com) используя учетные данные, предоставленные клиентом.

2. Ознакомиться с документацией по проекту. Изучить:
   - Структуру источников данных
   - Структуру целевых хранилищ данных
   - Требования к трансформации данных
   - Схемы данных источников и приемников
   - Требования к расписанию выполнения пайплайнов

3. Проверить доступ к ресурсам Azure Data Factory. Убедиться в наличии прав на:
   - Редактирование пайплайнов
   - Тестирование пайплайнов
   - Настройку триггеров
   - Доступ к источникам и приемникам данных

## Создание и настройка основных компонентов

1. Открыть Azure Data Factory в Azure Portal, выбрав соответствующий ресурс, или перейти напрямую через URL https://adf.azure.com.

2. Настройка Linked Services (связанных служб) для источников и приемников данных:
   - В интерфейсе ADF перейти «Author» → «Connections» → «Linked Services»
   - Нажать «New» для создания нового Linked Service
   - Выбрать тип службы в зависимости от источника/приемника (Azure SQL Database, Azure Blob Storage, Azure Data Lake Storage и т.д.)
   - Ввести необходимые параметры подключения (Server name, Database name, Authentication type и т.д.)
   - Проверить соединение кнопкой «Test connection»
   - Сохранить Linked Service

3. Создание Datasets (наборов данных) для описания структуры данных:
   - Перейти «Author» → «Connections» → «Datasets»
   - Нажать «New dataset»
   - Выбрать тип источника данных и формат
   - Настроить параметры набора данных (привязка к Linked Service, путь к файлам или имя таблицы)
   - Определить схему данных (импортировать из источника или задать вручную)
   - Для параметризации использовать выражения с параметрами (например, `@dataset().folderPath`)

## Реализация семи пайплайнов передачи данных

### Для каждого из семи пайплайнов выполнить следующие шаги:

1. Создание пайплайна:
   - Перейти «Author» → «Pipelines»
   - Нажать «New pipeline»
   - Задать понятное имя пайплайна в соответствии с документацией

2. Добавление и настройка Activity (действий) в пайплайн:
   - Перетащить нужные действия из панели Activities в область разработки пайплайна
   - Для простой копии данных использовать «Copy Data»
   - Для более сложных трансформаций рассмотреть:
     - Data Flow (для трансформации данных без кода)
     - Lookup (для получения метаданных или справочных данных)
     - ForEach (для итерации по коллекции)
     - If Condition (для логики ветвления)
     - Execute Pipeline (для модульного вызова других пайплайнов)

3. Настройка действия Copy Data:
   - Указать Source dataset (набор данных-источник)
   - Указать Sink dataset (набор данных-приемник)
   - Настроить Mapping (сопоставление) полей источника и приемника
   - При необходимости добавить фильтрацию данных в Source options
   - Настроить параметры производительности (например, degree of copy parallelism)

4. Настройка Data Flow (при необходимости сложных трансформаций):
   - Создать новый Data Flow или выбрать существующий
   - Настроить Source transformation (указать источник данных)
   - Добавить необходимые трансформации (Filter, Join, Aggregate, Derived Column и т.д.)
   - Настроить Sink transformation (указать приемник данных)
   - Включить Data Flow debugging для тестирования трансформаций

5. Настройка зависимостей между действиями в пайплайне:
   - Соединить действия в нужной последовательности с помощью стрелок
   - Добавить условия успеха/неудачи для создания ветвлений

6. Настройка параметров пайплайна:
   - Перейти на вкладку «Parameters»
   - Добавить необходимые параметры (имя, тип, значение по умолчанию)
   - Использовать параметры в действиях пайплайна через выражения (например, `@pipeline().parameters.paramName`)

7. Тестирование пайплайна:
   - Нажать «Debug» для запуска пайплайна в режиме отладки
   - Ввести значения параметров (если есть)
   - Проверить выполнение каждого действия в пайплайне
   - Проверить правильность передачи данных в целевое хранилище

### Настройка триггеров для автоматического запуска пайплайнов

1. Создание триггеров для каждого пайплайна:
   - Перейти на вкладку «Manage» → «Triggers»
   - Нажать «New» для создания нового триггера
   - Выбрать тип триггера:
     - Schedule trigger (для запуска по расписанию)
     - Tumbling window trigger (для периодической обработки данных без пропусков и дублирования)
     - Event-based trigger (для запуска при появлении новых данных)

2. Настройка Schedule trigger:
   - Указать частоту и интервал выполнения (ежечасно, ежедневно, еженедельно и т.д.)
   - Настроить время начала и окончания действия триггера
   - При необходимости настроить дополнительные параметры (дни недели, время суток и т.д.)

3. Привязка триггера к пайплайну:
   - В настройках триггера выбрать пайплайн для запуска
   - Указать значения параметров пайплайна (если они есть)
   - Сохранить и активировать триггер

## Тестирование и мониторинг

1. Полное тестирование всей цепочки пайплайнов:
   - Запустить каждый пайплайн в правильной последовательности
   - Проверить корректность данных в конечных хранилищах
   - Убедиться, что все трансформации выполняются корректно

2. Настройка мониторинга:
   - Перейти «Monitor» → «Pipeline runs»
   - Проверить статус выполнения пайплайнов
   - Для детального анализа выбрать конкретный запуск пайплайна и изучить ход выполнения каждого действия
   - При обнаружении ошибок перейти в режим редактирования пайплайна и внести необходимые корректировки

3. Настройка оповещений:
   - Перейти в Azure Portal → «Monitoring» → «Alerts»
   - Создать правила оповещений для критических ситуаций (ошибки пайплайнов, превышение времени выполнения и т.д.)
   - Настроить действия при срабатывании оповещений (email, SMS, webhook и т.д.)

## Завершение проекта

1. Документирование внесенных изменений:
   - Обновить проектную документацию в соответствии с внесенными изменениями
   - Описать архитектуру и взаимосвязи между компонентами
   - Задокументировать порядок и периодичность запуска пайплайнов

2. Внедрение в производственную среду:
   - Убедиться, что все пайплайны работают корректно
   - Активировать все необходимые триггеры
   - Проверить первые запуски в производственной среде

3. Обеспечение отказоустойчивости:
   - Настроить механизмы повторения в случае временных сбоев
   - Создать альтернативные потоки для обработки ошибок
   - Обеспечить механизм восстановления данных после сбоев

## Дополнительные рекомендации

1. Использовать функцию Integration Runtime для оптимизации производительности и безопасности, особенно при подключении к локальным источникам данных.

2. Применять параметризацию во всех компонентах (пайплайны, наборы данных, связанные службы) для гибкости и возможности повторного использования.

3. Использовать системные переменные ADF (такие как `@pipeline().RunId`, `@pipeline().TriggerTime`) для облегчения отладки и аудита.

4. При необходимости доработки сложных трансформаций рассмотреть использование Azure Databricks или Azure Functions в рамках пайплайнов ADF.

5. Настроить интеграцию с системой контроля версий (Git) для возможности отслеживания изменений и совместной работы над проектом.
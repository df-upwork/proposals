Ниже — **примерный** подход к более «умному» контролю лимита 80 000 токенов/минуту при обращениях к Anthropic API, вместо простого «попытка/задержка/трёхкратный ретрай». Суть в том, чтобы **до** каждого запроса:

1. Определять, сколько токенов **примерно** уйдёт на текущий запрос.
2. Узнавать у сервера (из заголовков в предыдущих ответах) или из нашей учётной структуры, сколько уже израсходовано токенов за «текущую» минуту и/или сколько осталось до reset.
3. Если видим, что с учётом нового запроса можем превысить лимит, — **ждать** до тех пор, пока «минутный счётчик» не обнулится или пока `x-ratelimit-reset-per-minute` не настанет.

Разумеется, приведённый код нужно адаптировать под ваш реальный скрипт и логику. Идея в том, что **перед** вызовом `self.client.messages.create(...)` мы сначала проверяем, а не превысим ли лимит. Если запрос всё же «проскочил», но потом сервер вернул `429 RateLimitError`, мы перехватываем её и ждём до следующего `reset-per-minute`.

### Шаги реализации

1. **Добавить структуру учёта** (к примеру, `self._last_headers` и т. д.), где мы храним последние заголовки с `x-ratelimit-*`, чтобы понимать, сколько «осталось» токенов в минуту и через сколько секунд наступит reset.
2. **Переопределить метод, который вызывает `self.client.messages.create(...)`**, чтобы:
   - перед вызовом оценивать, сколько токенов уйдёт;
   - смотреть на `x-ratelimit-remaining-per-minute` из предыдущего ответа (если есть);
   - если **remaining** меньше ожидаемых токенов, подождать до `x-ratelimit-reset-per-minute`;
   - выполнить запрос;
   - по результату **сохранить** новые заголовки `response.headers`.
3. **Если словили `anthropic.RateLimitError`** — прочитать (если пришли) заголовки из исключения или из `response`, вычислить время `reset-per-minute`, поспать подольше, потом повторить.

Ниже — упрощённый пример (псевдо)кода, дополняющий ваш `DomainAnalyzer` из `All_domains_process_v2.py`.  

```python
import anthropic
import time
import logging

logger = logging.getLogger(__name__)

class DomainAnalyzer:
    def __init__(self, api_key: str):
        self.client = anthropic.Client(api_key=api_key)
        self._last_ratelimit_headers = {}  # Чтобы хранить последние x-ratelimit-* заголовки

    def _sleep_until_rate_reset_if_needed(self, next_request_tokens: int):
        """
        Примерный метод, который смотрит на заголовки из предыдущего ответа
        (self._last_ratelimit_headers), чтобы понять, не превысим ли мы лимит
        при следующем запросе.
        """
        # Если ещё ни разу не делали запросов (нет заголовков) — выходим
        if not self._last_ratelimit_headers:
            return

        # Извлекаем нужные поля
        used_per_minute = int(self._last_ratelimit_headers.get("x-ratelimit-used-per-minute", 0))
        limit_per_minute = int(self._last_ratelimit_headers.get("x-ratelimit-limit-per-minute", 80000))
        remaining_per_minute = int(self._last_ratelimit_headers.get("x-ratelimit-remaining-per-minute", 80000))
        reset_in_seconds = float(self._last_ratelimit_headers.get("x-ratelimit-reset-per-minute", 0))

        logger.debug(
            f"Anthropic rate-limit usage so far this minute: used={used_per_minute}, "
            f"remaining={remaining_per_minute}, limit={limit_per_minute}, reset_in={reset_in_seconds}"
        )

        # Если ожидаемый next_request_tokens может превысить оставшиеся
        # (подход наивный: next_request_tokens vs remaining_per_minute)
        if next_request_tokens >= remaining_per_minute:
            logger.info(f"Next request needs ~{next_request_tokens} tokens, but only {remaining_per_minute} remain.")
            logger.info(f"Sleeping {reset_in_seconds:.2f} seconds until rate-limit reset...")
            time.sleep(reset_in_seconds + 0.5)  # Добавляем «чуть-чуть сверху»
    
    def _update_rate_headers(self, response):
        """ Сохранить x-ratelimit-* из ответа Anthropic, чтобы знать usage/limit/reset. """
        if not hasattr(response, "headers"):
            return
        # Предположим, response.headers — это словарь {str -> str} или подобное
        # (В реальном anthopic API надо убедиться, что это response.http_status.headers)
        for hname, hval in response.headers.items():
            hname_lower = hname.lower()
            if hname_lower.startswith("x-ratelimit-"):
                self._last_ratelimit_headers[hname_lower] = hval
    
    def _estimate_tokens(self, prompt_text: str) -> int:
        """
        Очень упрощённо оценим кол-во токенов, 
        например, 1 токен = ~4 символа. Или используем более точный токенайзер.
        """
        return max(1, len(prompt_text) // 4)

    def _process_chunk_with_instances(
        self, chunk: str,
        prompt: dict,
        system_prompt: str,
        previous_results: dict,
        is_multi_instance: bool
    ):
        # Примерно оцениваем будущий расход токенов
        # (user_message + system_prompt + вспомогательное)
        user_message = f"Analyze this document section: {chunk} ..."
        estimated_tokens = self._estimate_tokens(user_message) + self._estimate_tokens(system_prompt) + 200

        # Перед реальным запросом проверим, не надо ли поспать
        self._sleep_until_rate_reset_if_needed(estimated_tokens)

        # Теперь делаем запрос — обернём в try/except
        max_retries = 5
        attempt = 0
        while True:
            try:
                response = self.client.messages.create(
                    model="claude-2",
                    max_tokens=1024,
                    temperature=0,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_message}],
                )
                # Парсим заголовки Anthropic, чтобы обновить usage
                self._update_rate_headers(response)
                
                # ... Далее логика обработки JSON ...
                return {"some_parsed_data": True}

            except anthropic.RateLimitError as e:
                # Попробуем аккуратно подождать.
                attempt += 1
                # Считаем, что из e или из e.response.headers можно выдрать x-ratelimit-reset-per-minute
                reset_in = 10  # fallback
                if hasattr(e, "response") and hasattr(e.response, "headers"):
                    reset_in_str = e.response.headers.get("x-ratelimit-reset-per-minute")
                    if reset_in_str:
                        reset_in = float(reset_in_str)
                if attempt < max_retries:
                    logger.warning(f"Rate limit error, attempt={attempt}, waiting ~{reset_in} sec.")
                    time.sleep(reset_in + 1)
                else:
                    logger.error(f"Rate limit error, attempts exceeded. Re-raising.")
                    raise

            # end while True

        # end _process_chunk_with_instances
```

**Ключевые моменты:**
1. Мы **до отправки** запроса оцениваем примерное количество токенов (`_estimate_tokens(...)`) и вызываем `_sleep_until_rate_reset_if_needed(...)`, где смотрим заголовки `x-ratelimit-*` (из предыдущих ответов) и при необходимости ждём.
2. **После** каждого запроса (если успех) мы парсим заголовки из ответа (`_update_rate_headers`) и сохраняем их во внутреннем словаре `self._last_ratelimit_headers`.
3. Если всё же неожиданно прилетел `RateLimitError`, мы пытаемся взять `e.response.headers["x-ratelimit-reset-per-minute"]`, подождать нужное время и попробовать заново.  
4. Число `max_retries` сделано побольше, потому что теперь мы более аккуратно ждём (пока не наступит reset).

В итоге такой подход даёт возможность **не «тупо»** делать экспоненциальную задержку, а реально ориентироваться на лимит 80 000 токенов/мин, заглядывая в заголовки Anthropic API и делая «предварительные» паузы, если видим, что можем превысить лимит при ближайшем запросе.
1) I am currently developing an AI-based legal system where I have already tackled a similar challenge: a similar chatbot must, on the one hand, be based on a general-purpose LLM, and on the other, it must employ precise knowledge of a specific, narrow domain, which the underlying LLM does not possess sufficiently well, and sometimes does not know at all.
2) На сегодня (конец декабря 2024 года) лучшим решением для вашего случая (B2B customer support in eCommerce) является сочетание 2 технологий: 
2.1) RAG (Retrieval Augmented Generation): 
*) https://en.wikipedia.org/wiki/Retrieval-augmented_generation
*) https://help.openai.com/en/articles/8868588
*) https://www.anthropic.com/news/contextual-retrieval#a-primer-on-rag-scaling-to-larger-knowledge-bases
2.2) Function Calling (OpenAI) / Tools (Anthropic):
https://platform.openai.com/docs/guides/function-calling
https://docs.anthropic.com/en/docs/build-with-claude/tool-use
https://github.com/anthropics/courses/blob/master/tool_use/01_tool_use_overview.ipynb#repos-sticky-header
3) В вашем случае технологии 2.1 и 2.2 лучше всего будут работать именно совместно.
3.1) В B2B customer support in eCommerce эти технологии обычно сочятают следующим образом:
3.1.1) Шаг RAG: LLM через ваш «retriever» (или через встроенный RAG-подход) получает релевантные документы (тексты, описания товаров, юридические нормы и т.д.).
3.1.2) Шаг function calling: Если нужно выполнить конкретные действия (например, оформить заказ, посчитать доставку), LLM «решает», что пора вызывать функцию placeOrder(userId, itemId, quantity) и т.д.
3.2) Пример.
Пользователь спрашивает: «Какова цена товара Х на оптовом складе B2B?». 
AI-модель:
3.2.1) Через RAG находит описание товарных позиций (прайслисты, тех. документацию).
3.2.2) Затем она через function calling вызывает API Magento, чтобы проверить остатки, актуальную цену, скидки и т. д.
3.2.3) Затем она отвечает, ссылаясь и на retrieved-данные, и на результаты вызова функции.
4) Как реализовать RAG в вашем случае?
4.1) Document your business knowledge in the formats commonly accepted today for LLM: Markdown, XML, PDF, images.
4.2) Convert the data from step 3.1 into embeddings:
4.2.1) https://en.wikipedia.org/wiki/Word_embedding
4.2.2) https://platform.openai.com/docs/guides/embeddings
4.2.3) https://docs.anthropic.com/en/docs/build-with-claude/embeddings
4.3) Save the embeddings in a vector database:
4.3.1) https://en.wikipedia.org/wiki/Vector_database
4.4) Retrieval: Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query
4.5) Augmentation: The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query
4.6) Generation: Finally, the LLM can generate output based on both the query and the retrieved documents
5) Как реализовать function calling в вашем случае?



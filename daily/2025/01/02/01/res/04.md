Ниже приводится один из возможных способов реализовать учёт расхода токенов в минуту и избежать превышения лимита Anthropic (80 000 токенов в минуту). Суть решения:

1. **Отслеживать (примерно) объём токенов**, отправляемых в каждом запросе и получаемых в ответе.
2. **Вести скользящее окно** (rolling window) в 60 секунд, где для каждого запроса сохраняем `(timestamp, used_tokens)`.
3. **Перед новым запросом** проверяем, не превысим ли мы 80 000 токенов в сумме за последние 60 секунд (учитывая и будущие токены на ответ). Если превысим — подождать некоторое время (sleep), пока старые запросы «выйдут» за 60‑секундное окно.

### Пример кода

Ниже — условная «облегчённая» версия кода, как можно встроить этот механизм в `All_domains_process_v2.py`. Обратите внимание:

- Код предполагает, что мы можем вытащить реальное число использованных токенов из заголовков ответа Anthropic (иногда они передаются в `response.response.headers`, например `x-request-tokens-used` — точное имя может отличаться).
- Если же таких данных нет — можно **приблизительно** считать количество токенов (например, `n = len(prompt_text)//4 + reserve`) и вести учёт «по оценке», а не «по факту».
- Код ниже только демонстрирует идею. Вам придётся аккуратно интегрировать его в ваш класс (например, в метод `_process_chunk_with_instances`) и подстроить под ваш формат извлечения заголовков.

```python
import time
import anthropic
import logging

class DomainAnalyzer:
    def __init__(self, api_key: str):
        self.client = anthropic.Client(api_key=api_key)
        self.metamodel_data = {}
        # ...
        
        # Храним историю (timestamp, tokens_used) последних 60 секунд
        self.token_usage_window = []
        # Лимит Anthropic — 80_000 токенов в минуту
        self.TOKEN_RATE_LIMIT = 80_000

    def _cleanup_token_window(self, now: float) -> int:
        """
        Удаляем из token_usage_window все записи старше 60 секунд.
        Возвращает сумму оставшихся токенов за последние 60 секунд.
        """
        one_minute_ago = now - 60
        new_window = []
        for (ts, used) in self.token_usage_window:
            if ts > one_minute_ago:
                new_window.append((ts, used))
        self.token_usage_window = new_window
        
        return sum(used for (ts, used) in self.token_usage_window)

    def _wait_for_token_budget(self, tokens_needed: int):
        """
        Убедиться, что в ближайший момент мы не пробьём лимит 80k токенов/минуту.
        Если пробиваем — ждём, пока не освободится место.
        """
        while True:
            now = time.time()
            used_in_last_minute = self._cleanup_token_window(now)
            
            if used_in_last_minute + tokens_needed <= self.TOKEN_RATE_LIMIT:
                # можно отправлять
                break
            else:
                # Ждём немного, чтобы «старые» записи вышли из окна
                logging.debug(
                    f"Anthropic token budget exceeded: need {tokens_needed}, "
                    f"used in last minute {used_in_last_minute}. Waiting 1s..."
                )
                time.sleep(1)

    def _estimate_tokens(self, system_prompt: str, user_message: str, max_tokens_response: int) -> int:
        """
        Приблизительно оцениваем, сколько токенов уходит в Prompt + сколько ожидать в ответе.
        Это грубая оценка: len(...)//4 для текста.
        """
        text_prompt = system_prompt + user_message
        approximate_prompt_tokens = len(text_prompt)//4
        # Прибавляем ожидаемое кол-во токенов в ответе
        return approximate_prompt_tokens + max_tokens_response

    def _process_chunk_with_instances(
        self, 
        chunk: str, 
        prompt: dict, 
        system_prompt: str,
        previous_results: dict,
        is_multi_instance: bool
    ):
        user_message = f"Analyze this document section:\n{chunk}\n"
        max_tokens_req = 2048  # Например, уменьшаем с 4096 до 2048
        # Сначала оцениваем, сколько токенов потребуется
        tokens_needed = self._estimate_tokens(system_prompt, user_message, max_tokens_req)
        
        # Ждём, если лимит 80k/мин «забит»
        self._wait_for_token_budget(tokens_needed)
        
        for attempt in range(3):
            try:
                response = self.client.messages.create(
                    model="claude-2.0", 
                    max_tokens=max_tokens_req,
                    temperature=0,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_message}],
                )
                
                # Запрашиваем реальное usage из заголовков (если доступно)
                # Пример! Точное название может отличаться
                used_tokens_header = response.response.headers.get("x-request-tokens-used")
                if used_tokens_header is not None:
                    try:
                        used_tokens = int(used_tokens_header)
                    except ValueError:
                        used_tokens = tokens_needed  # fallback
                else:
                    used_tokens = tokens_needed  # если не нашли
                
                # Сохраняем этот usage в окно
                now = time.time()
                self.token_usage_window.append((now, used_tokens))
                
                # Далее парсим ответ...
                result = self._process_response(response.content[0].text)
                
                # и т.д. ...
                return result

            except anthropic.RateLimitError:
                # если всё-таки словили RateLimit, увеличим задержку
                delay_s = 5 * (2 ** attempt)
                logging.warning(f"Rate limit error, waiting {delay_s} seconds...")
                time.sleep(delay_s)

        # Если 3 попытки не привели к успеху — бросаем исключение
        raise RuntimeError("Exceeded max retries due to RateLimitError")
```

### Комментарии к решению

1. **Сбор usage из заголовков**: если Anthropic возвращает в `response.response.headers` информацию о том, сколько токенов ушло/вернулось, то мы можем использовать реальное число `used_tokens`. Если нет, придётся **делать оценку**: количество токенов во входном тексте (`prompt + system + user_message`) плюс `max_tokens`.  
2. **Скользящее окно**: каждые раз перед отправкой запроса вызывается `_wait_for_token_budget(tokens_needed)`, которая смотрит, сколько уже было израсходовано за последние 60 секунд, и если сумма + нужные токены > 80 000, — ждёт. После успешного запроса добавляем `(time.time(), used_tokens)` в `token_usage_window`.  
3. **Уменьшение `max_tokens`**: зачастую помогает не превысить лимит, потому что чем меньше максимально допустимый объём ответа, тем меньше суммарные токены «за минуту».  
4. **Уменьшение чанка**: всё равно может понадобиться, если у вас слишком большие куски документа. В сочетании с учётом токенов это даст наилучший результат.

Таким образом, если вы аккуратно вставите описанный кусок кода (или адаптируете под вашу логику), ваш скрипт будет уважать лимит 80 000 токенов/мин Anthropic, дожидаясь «окна» при необходимости.
1)
Добавить пункт 6:
6) Ответ на вопрос «Have you implemented a RAG pipeline (query → embedding → vector search → ChatGPT answer)? Please explain the stack you used (LangChain/LlamaIndex/etc.)».
6.1) Да, я многократно реализовывал RAG-конвейеры, и ниже я опишу стек и конвейер, предлагаемые для вашего проекта в рамках описанной выше архитектуры.
6.2) Технологический стек.
6.2.1) Backend (RAG Runtime Service) будет реализован на Python или Node.js, в соответствии с вашими требованиями.
6.2.2) В качестве векторной базы данных будет использоваться Elasticsearch (или OpenSearch), преимущества которого я обосновал в пункте 2.
6.2.3) В качестве фреймворка оркестрации я рекомендую использовать ``LangChain``.
6.2.4) ``LangChain`` предпочтительнее ``LlamaIndex`` для Production-систем E-commerce благодаря его зрелой экосистеме.
6.2.5) Эта экосистема включает ``LangSmith`` (для детального мониторинга и отладки) и ``LangServe`` (для развертывания высокопроизводительного API с поддержкой Streaming).
6.2.6) В качестве LLM будет использоваться OpenAI API (рекомендуемая модель ``gpt-4o``).
6.3) RAG Pipeline.
6.3.1) Query Embedding (Векторизация запроса).
6.3.1.1) Входящий текстовый запрос пользователя преобразуется в плотный вектор (embedding) с помощью OpenAI API (например, модель ``text-embedding-3-small``).
6.3.1.2) Критически важно использовать ту же модель, что и на этапе индексации каталога, для обеспечения консистентности векторного пространства.
6.3.2) Vector Search (Поиск в Elasticsearch).
6.3.2.1) Для достижения максимальной релевантности в контексте E-commerce будет использоваться стратегия гибридного поиска (Hybrid Search).
6.3.2.2) Hybrid Search комбинирует семантический векторный поиск (ANN), который улавливает намерение пользователя, с традиционным полнотекстовым поиском (BM25), который эффективен для точных совпадений (например, SKU или бренды).
6.3.2.3) Для объединения результатов будет использоваться метод Reciprocal Rank Fusion (RRF).
6.3.2.4) RRF позволяет эффективно комбинировать ранги документов без сложной настройки весовых коэффициентов (boosts).
6.3.2.5) Запрос выполняется к выделенному RAG-индексу в Elasticsearch, содержащему предварительно подготовленные данные и векторы продуктов.
6.3.3) ChatGPT Answer (Генерация ответа).
6.3.3.1) Наиболее релевантные документы, найденные на этапе поиска, извлекаются и формируют контекст для LLM.
6.3.3.2) Формируется промпт, включающий инструкции (System message), контекст о продуктах и исходный запрос пользователя.
6.3.3.3) Prompt Engineering используется для минимизации галлюцинаций (Grounding) и обеспечения выполнения бизнес-требований.
6.3.3.4) System message будет явно инструктировать модель основывать ответы исключительно на предоставленном контексте и обязательно включать цены и ссылки (URL) на продукты.
6.3.3.5) Промпт отправляется в OpenAI Chat Completions API.
6.3.3.6) Параметр ``temperature`` будет установлен в низкое значение (0.0–0.1) для обеспечения точности и детерминированности ответов, основанных на фактах каталога.
6.3.3.7) Для улучшения пользовательского опыта (UX) в On-site Chatbot Widget будет использоваться потоковая передача ответа (Streaming).
1) I am currently developing an AI-based legal system, where I have already faced a similar challenge: bridging a general-purpose LLM with domain-specific knowledge that the base model does not inherently possess.
2) By December 2024, the recommended solution for your case (customer support) is a combination of 2 technologies:
2.1) RAG (Retrieval Augmented Generation):
- https://en.wikipedia.org/wiki/Retrieval-augmented-generation
- https://help.openai.com/en/articles/8868588
- https://www.anthropic.com/news/contextual-retrieval#a-primer-on-rag-scaling-to-larger-knowledge-bases
2.2) Function Calling (OpenAI) / Tools (Anthropic):
- https://platform.openai.com/docs/guides/function-calling
- https://docs.anthropic.com/en/docs/build-with-claude/tool-use
- https://github.com/anthropics/courses/blob/master/tool_use/01_tool_use_overview.ipynb#repos-sticky-header

3) Технология RAG предназначена для дополнения базовой LLM (в вашем случае — customer service chatbot) данными конкретной предметной области.
RAG по принципу своей работы — это как маленький личный Google вашей компании, только предназначенный не для людей, а для ИИ.
RAG, как Google, индексирует данные заранее. 
RAG хорош своей универсальностью: он способен индексировать всё подряд единообразно (как Google) и не требует программирования каждого конкретного случая.
RAG хорош для данных, которые не меняются между циклами индексации (хотя бы 1 рабочий день): 
*) документация к вашим товарам
*) юридические документы: ваши контракты с контрагентами (поставщиками, покупателями, партнёрами)
*) база знаний ответов на типичные вопросы пользователей (FAQ)
4) Технология «Function calling» / «tools» способа выполнять 2 важные задачи:
4.1) Она тоже, как и RAG, способна предоставлять чатботу точные данные, но по-другому и на другом этапе: 
4.1.1) Она способна добывать данные более сложными способами, чем RAG, и из более сложных источников, потому что каждый подобный сценарий добычи данных программируется программистом индививидуально.
В частности, в вашем случае я могу запрограммировать технологию «Function calling» / «tools» для предоставления чатботу данных: 
*) из базы данных вашего интернет-магазина (Magento)
*) из других используемых вашей компанией IT-систем
*) из IT-систем ваших партнёров (если реализовать с ними интеграцию)
4.1.2) Технология «Function calling» / «tools» добывает данные в реальном времени (при ответе на конкретный запрос конкретного пользователя).
Это ключевое отличие от RAG.
Таким образом, технология «Function calling» / «tools» важна там, где данные могут меняться очень быстро:
*) состояние конкретного заказа
*) складские остатки товаров
*) цены на товары (в некоторых бизнес-сценариях)
4.2) Помимо добычи актуальных данных в реальном времении для базовой LLM, технология «Function calling» / «tools» способна выполнять вторую важную задачу: она способна так же в реальном времени по запросу пользователя LLM манипулировать данными (выполнять задачи пользователя).
В частности, в вашем случае я могу запрограммировать технологию «Function calling» / «tools» на все те действия, которые ваши покупатели сейчас совершают через витрину Magento или через свой личный кабинет в Magento:
*) принятие нового заказа от покупателя и регистрацию иго в Magento
*) регистрацию нового адреса (доставки или платёжного), редактирование прежнего адреса
*) редактирование ранее размещённого заказа
*) отмена заказа
*) повторение прежнего заказа
*) задача вопроса о товаре
*) добавление отзыва на товар

5) How to implement RAG in your case?
5.1) Document your business knowledge in the formats commonly accepted today for LLM: Markdown, XML, PDF, images.
5.2) Convert the data from step 3.1 into embeddings:
- https://en.wikipedia.org/wiki/Word_embedding
- https://platform.openai.com/docs/guides/embeddings
- https://docs.anthropic.com/en/docs/build-with-claude/embeddings
5.3) Save the embeddings in a vector database: https://en.wikipedia.org/wiki/Vector_database
5.4) Retrieval: given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query
5.5) Augmentation: the model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query
5.6) Generation: finally, the LLM can generate output based on both the query and the retrieved documents
6) How to implement function calling in your case?
6.1) Identify the specific functions (API methods) that the chatbot could call within your Magento environment.
6.2) Instruct the LLM when to call each function.
For example: «If the user wants to place an order, call the `placeOrder` function with these parameters. Do not generate a user-facing response at that step—just generate the JSON function call.»
6.3) When the LLM returns the JSON specifying the function name and arguments (e.g., `placeOrder(userId, productId, quantity)`), your system (usually, a module in Magento) triggers the appropriate Magento functionality, and then sends the response from Magento back to the LLM.
6.4) Finally, the LLM incorporates these results into a user-friendly response (e.g., «Your order ORD-1234 has been successfully placed»).
## 1.
Потенциальный клиент опубликовал на Upwork следующий проект:
### 1.1. Title
Azure Data Factory expert needed for immediate short task - must start today

### 1.2. Description
We have a set of seven short data pipelines taking data from one source and inserting it into another, using ADF. (Note classic ADF, not Fabric.)
Due to illness, our senior data engineer is not able to complete the work, and we need an urgent replacement. Documentation is thorough, and although the flows are a little complex, this should be a nice quick job for someone with ADF experience.
You must be highly proficient with ADF. Please provide examples of recent projects. You must be able to start work today.

### 1.3. Tags
azure data factory

### 1.5. Questions
#### 1.5.1.
Please confirm you have up-to-date, expert experience with classic ADF. Please do not apply if you cannot say yes to this requirement.
#### 1.5.2.
Describe your recent experience with similar projects
#### 1.5.3.
Please list any certifications related to this project

## 2. Информация о клиенте
### 2.1. Местоположение
United Kingdom
London

## 2.2. Характеристики компании
### 2.2.1. Сектор экономики
неизвестно
### 2.2.2. Количество сотрудников
Mid-sized company (10-99 people)

## 2.3. Характеристики учётной записи на Upwork
### 2.3.1. Member since
Mar 21, 2021
#### 2.3.2. Hire rate (%)
100
#### 2.3.3. Количество опубликованных проектов (jobs posted)
6
#### 2.3.4. Total spent
$83K
#### 2.3.5. Количество оплаченных часов в почасовых проектах
1990

## 3. Обозначения
### 3.1. `C`
Клиента буду называть `C`.

### 3.2. `P`
Проект `C` буду называть `P`.

### 3.3. `Ps`
Множество подобных `P` проектов буду обозначать `Ps`.

## 4. Ссылки на официальную документацию
### 4.1.
Я тебя спросил:
~~~
Дай 2 ссылки:
- на официальную документацию «classic ADF»
- на официальную документацию «Fabric ADF»
~~~

### 4.2.
Ты ответил:
~~~
**Ниже приведены 2 ссылки на официальную документацию от Microsoft по двум разным версиям ADF:**

1. **Classic ADF (Azure Data Factory)**  
   [Документация по Azure Data Factory](https://learn.microsoft.com/azure/data-factory/)  

2. **Fabric ADF (Data Factory в Microsoft Fabric)**  
   [Документация по Data Factory в Microsoft Fabric](https://learn.microsoft.com/fabric/data-factory/)
~~~

## 5. `ADF`
«Azure Data Factory» / «Classic ADF» (пункт 1 твоего ответа пункта 4.2) буду обозначать `ADF`.
https://learn.microsoft.com/azure/data-factory

## 6. `FDF`
«Data Factory в Microsoft Fabric» / «Fabric ADF»  (пункт 2 твоего ответа пункта 4.2) буду обозначать `FDF`.
https://learn.microsoft.com/fabric/data-factory

## 10. Мой потенциальный ответ клиенту (редакция 1)
### 10.1.
Я тебя спросил:
~~~
## 10. Мой потенциальный ответ клиенту (редакция 1)
Я хочу ответить клиенту так:
~~~
I can do it today. Usually it is done as follows:
1) «Create a resource» → «Integration» → «Azure Data Factory»
2) Создать «Linked Services» для источника данных и приёмника («Manage» → «Linked Services»)
3) Создать «Datasets» для каждого источника и приёмника
4) Создать «Pipeline»:
4.1) В разделе «Author» → «+» → «Pipeline».
4.2) Добавить «Activities» типа «Copy Data» или «Data Flow» (если требуется трансформация).
4.3) Настроить входной и выходной «Dataset», указав:
- Источник (например, «Azure SQL Database Dataset»).  
- Приёмник (например, «Azure Blob Storage Dataset»).  
4.4) В разделе «Settings» для «Copy Data» указать параметры копирования (метод, параллельность, поведение при ошибках).
4.5) При необходимости добавить другие «Activities» (например, «Lookup», «Stored Procedure», «Script», «Databricks Notebook» и так далее) с учётом потребностей проекта.
5) Проверить и протестировать «Pipeline»:
5.1) Нажать «Debug» в верхней части экрана, чтобы запустить «Pipeline» в режиме отладки.
5.2) Проверить журнал выполнения в «Output» или перейти в «Monitor» → «Pipeline runs».
5.3) Убедиться в корректном переносе и/или трансформации данных: в источнике должна уменьшиться/измениться выборка (если операции чтения синхронные), в приёмнике должны появиться ожидаемые данные.
6) Добавить «Triggers» для автоматического запуска
7) Опубликовать все изменения
8) Настроить мониторинг и оповещения
~~~

## 11. Твоя задача
Есть ли в моём ответе фактические ошибки?
Есть ли в моём ответе логические ошибки?
Упустил ли я в ответе нечто важное?
~~~

### 10.2.
Ты ответил:
~~~
Ни фактических, ни явных логических ошибок в вашем тексте нет: вы описали классическую последовательность действий при настройке пайплайнов в ADF, упомянули ключевые шаги (Linked Services, Datasets, Pipelines, Debug/Monitor, Triggers, Publish). Такой общий чек-лист действительно соответствует типичному сценарию разработки в Azure Data Factory.

Однако есть несколько нюансов, которые можно улучшить или учесть:

1. **Уточнить, нужен ли вообще шаг «Create a resource»**  
   - В вашем пункте 1 описано создание ресурса ADF в Azure Portal. Если у клиента уже есть развернутый Data Factory (а он, скорее всего, уже создан и настроен, раз инженеры на проекте работали), то этот шаг может не потребоваться.  
   - Можно оставить его как «проверить, что ресурс ADF создан и доступен», чтобы не выглядело, будто вы всегда начинаете с нуля.

2. **Тест подключения в Linked Services**  
   - В вашем ответе вы не упомянули проверку подключения (кнопкой «Test connection») при создании Linked Services. Это не ошибка, но обычно полезно уточнить в тексте, что соединение следует проверить сразу, чтобы исключить проблемы с доступами/credentials.

3. **Трансформация не всегда «уменьшает» выборку в источнике**  
   - В пункте 5.3 вы пишете: «... в источнике должна уменьшиться/измениться выборка». Это верно лишь в тех случаях, когда при чтении мы фильтруем данные или выполняем «DELETE» после копирования (что не всегда делается).  
   - Сам по себе «Copy Data» не удаляет данные из источника. Он только читает. Если речь о трансформации с фильтрацией (или о сценариях, где исходные данные действительно удаляются), то да — но лучше аккуратнее сформулировать. Иначе может создаться впечатление, что чтение само по себе что-то меняет в источнике.

4. **Не упомянута работа с параметрами и выражениями**  
   - Многие проекты ADF используют параметризацию (pipeline parameters, dataset parameters) и выражения для динамической подстановки путей, имён таблиц, и т.д.  
   - Это не обязательно, но иногда клиенту важно знать, что вы умеете настраивать гибкую параметризацию.

5. **DevOps/Git интеграция** (необязательно, но часто встречается)  
   - Если проект требует версионного контроля и CI/CD (а у клиента могут быть такие требования, учитывая срочность и командную работу), можно коротко упомянуть, что при необходимости вы можете настроить Git-интеграцию в ADF.  
   - Это не обязательно, но некоторые заказчики любят видеть, что фрилансер знаком с DevOps-подходом к ADF.

6. **Языковая консистентность**  
   - В тексте смешаны русский и английский. Часто это нормально, особенно если в интерфейсе ADF пункты на английском, а пояснения на русском. Но если хотите, чтобы текст выглядел более цельно, можно выдержать его либо полностью на английском, либо полностью на русском с англоязычными терминами в скобках.

В остальном структура ответа правильная, по сути вы кратко и верно изложили основные шаги:  
1. (Проверить/создать) ресурс ADF  
2. Linked Services  
3. Datasets  
4. Pipelines и Activities  
5. Debug/Monitor  
6. Triggers  
7. Publish all  
8. Настройка мониторинга и оповещений  

**Итого**: можете смело использовать такой ответ в переписке — он корректный и охватывает типовые действия. При желании добавьте пару уточняющих моментов (тест подключений, параметризация), и ответ будет выглядеть ещё более профессионально.
~~~

## 11. «Не упомянута работа с параметрами и выражениями»
В пункте 10.2 ты писал:
```
4. **Не упомянута работа с параметрами и выражениями**  
   - Многие проекты ADF используют параметризацию (pipeline parameters, dataset parameters) и выражения для динамической подстановки путей, имён таблиц, и т.д.  
   - Это не обязательно, но иногда клиенту важно знать, что вы умеете настраивать гибкую параметризацию.
```
Предложи конкретные правки к моей инструкции пункта 10.1 для устранения именно этого конкретного твоего замечания (не других).
Полностью инструкцию не переписывай: вместо этого предложи конкретные точечные правки.
Для подготовки правок используй свой ответ пункта 10.2.
